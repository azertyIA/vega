**Deep Learning** is a subfield of machine learning that employs neural networks with multiple layers that extracts patterns from data, without the need for feature engineering.

Feature engineering did exist to capture patterns like edges using gradients to make it more comfortable for typical machine learning systems to use (they used histograms back then).

Deep learning takes answers and data and extracts the rules automatically though. Unfortunately, deep learning needs large memory access and specialized accelerators to do the heavy nature of matrix multiplication. This is usually limited by memory bandwidth rather than processing power. That's why it's imperative that the data stay as close to the processors as possible. 

This is also why GPUs, with onboard memory, are better geared towards these kind of operations. Lowering memory requirements through quantization and pruning are easy software level optimizations to make. However to actually scale, Deep learning consumes exponentially more resources as they grow in complexity. Deep Learning stems from four biological principal:
- adaptive learning: neural connections built from experiences
- parallel processing: lots of neurons work in parallel
- pattern recognition: excels at finding patterns in noisy data
- energy efficiency: we only consume twenty watts

Each neuron contains some resemblance to an actual neuron:
- node: the basic computational atom
- inputs: receipts of signals collecting information
- weights: modulates the strengths of connections between nodes
- summation: the inputs are added up in a way
- output: activation is applied to the summation to get a normalized signal

The architecture of a neural network determines how information flows through the system to the output. For example, you'd need around a hundred thousand parameters to understand numbers from 784 inputs. 784 to 128 to 64 to 10 if you want to make it in a GPU.

Non-linear activation functions are absolutely necessary for all of this to work. Each input needs some storage in memory for processing.
$$z_j=\Sigma (x_iw_{ij})+b_{ij}$$
After this, you apply a non-linear activation to normalize the result to something workable. One big issue with many activation functions is the fact that gradients vanish for high values.
- $\sigma=1/(1+e^{-x})$
- $\tanh=(e^x-e^{-x})/(e^x+e^{-x})$
- $ReLU=\max(0,x)$
- $softmax=e^z/\Sigma e^z$

Instead of all this, we can just use matrix multiplication:
$$h'=f(h^TW+b)$$
The design of neural network topology centers on the width, depth and interconnection. Each layer (depth) creates means to build increasingly complex features, but are difficult to train. Wider layers help encode more information, but more parameters are needed, obviously. Between layers are going to get you $n_1\times n_2$ different weights, and there are $n_2$ different biases before getting to the activation. Old NNs used to have just one layer, so things like XORs were impossible because they require at least some form of non-linearity.

To actually learn from data, each batch needs to go through some operations:
- data -> predictions (forward compute)
- predictions -> loss (loss function)
- loss -> weight adjustments (gradient)
- weight adjustments -> new model (change weights)

### Forward Compute
To get the next set of activations, each layer needs to do two things: a linear transformation followed by a non-linear activation.
$$\vec Z=W\vec A+\vec b$$
$$\vec A'=f(\vec Z)$$
And these nest upon deeper and deeper layers.

> The top equation is actually called an affine transformation and can be written as one big matrix multiplication by simply appending a $1$ to the end of the activation vector.

At the end of all this you have some output which you can apply some soft max function to get the probability you choose something. Memory blows up as you increase layers due to the fact that there are so many parameters. You can usually split this up into batches.

By the last layer you ideally want to apply some SoftMax or sigmoid so you can better read your data either as a 1 or a 0.
$$\hat p=\sigma(W\vec A+\vec b)$$
$$\text{Softmax}(x)=e^{x}/\Sigma e^{x_i}$$
### Loss Function

To make good predictions, we should be able to know what a good prediction is. When training with batches of data, we usually compute the average loss across all samples in the batch, $\hat y$ is the prediction:
$$L=\frac1B\Sigma L(\hat p,\vec y)$$
Sometimes you can have losses that mask out for a certain answer:
$$L(\hat p,\vec y)=-\Sigma y\log(\hat p)$$
But taking the log of zero is kind of dangerous, so we add some definite offset $\epsilon$:
$$L(\hat p,\vec y)=-\Sigma y\log(\hat p+\epsilon)$$
When using this with the SoftMax output, it's important to look at the derivative for very soon.
$$\vec{\partial_pL}=-\partial_p(\Sigma y\log(\hat p))=\partial_p(\Sigma y\log(e^p/\Sigma e^p))$$
$$=-\partial_p(\Sigma y\log(e^p)-\Sigma y\log(\Sigma e^p))$$
$$=-\partial_p(\Sigma yp-\Sigma y\log(\Sigma e^p))$$
$$=-y+\Sigma y\partial_p\log(\Sigma e^p)$$
$$=-y+\Sigma y(1/\Sigma e^p)(\partial_pe^p)$$
$$=-y+(1/\Sigma e^p)(\partial_pe^p)\Sigma y$$
$$=-y+\hat p$$
### Back Compute
Just go back from the answer and see how much each parameter contributes to the loss of the entire model, and attribute accordingly:
$$\partial_{A}L=(\partial_{A}L)(\partial_{A'}A)$$
Assuming the loss function is made up of a sum, and the output is $O$:
$$L=\Sigma L'(O^i,y^i)$$
$$\hat p=P(Z^i)=P(\vec W^i\cdot\vec X+b^i)=P(\Sigma (W^{ij}X^j)+b^i)$$
$$X^i=\sigma(\vec W'^i\cdot\vec X'+b'^i)=\sigma(\Sigma (W'^{ij}X'^j)+b'^i)$$
To find the impact a parameter has on a layer:
$$\partial_{Bj}X_j=\sigma'(\dots)=\delta_j\to\vec\delta$$
$$\partial_{Wij}X_j=\sigma'(\dots)X_i'=\delta_j X_i'\to\vec\delta\vec X'^T$$
$$\partial_{X'i}X_j=\Sigma_n\sigma_n'(\dots)W_{in}=\delta_j W_i'\to W^T\vec\delta$$
All chain rule products are propagated through Hadamard products ($\odot$) because the elements are the same size for both vectors (they get passed through $\vec\delta$). The loss function also sees propagation through the first $\vec\delta$ with its factor being $\hat p-y$ for instance.

