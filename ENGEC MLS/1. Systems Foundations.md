The core of any machine learning (ML) system consists of three components forming a triangular dependency:
1. **algorithms**: mathematical models and methods that learn patterns from data to make predictions, inferences or decisions.
2. **data**: processes and infrastructure for collecting, storing and serving data.
3. **computation**: hardware and software infrastructure that enables large-scale rapid computation at a scale (GPUs)

No single element can function in isolation, as algorithms require data, datasets require algorithms (questionable) to store and serve, and computation requires both to do anything meaningful.

This is different from normal software as that has immediate feedback when something goes wrong. However, with MLs it will take generations of being in accurate to even diagnose an issue. It will never break, but it will be poor. Thus, AI success has been grounded in systems engineering.
> Short-term improvements are consistently surpassed by general methods to leverage massive computational resources. Don't rely on encoding humans.

Due to Amdahl's Law: $A=1/(1-P)$, the speed up from parallelization is limited by how much of the program is parallelizable, so much of the bottleneck in ML is memory bandwidth. Loading 1GB from VRAM costs the same energy as multiplying 32-bit numbers 1000 times. 

There has been a big AI boom because of:
1. massive datasets (scrapers)
2. algorithmic breakthroughs (transformers)
3. hardware acceleration (GPUs and cloud computing)

Originally it started with symbolic AI, turning intelligence into algebra from enough parsing. Then came the rule-based classifiers. Then came statistical learning:
```
Rule-based (1980s):
IF contains("viagra") OR contains("winner") THEN spam

Statistical (1990s):
P(spam|word) = (frequency in spam emails) / (total frequency)

Combined using Naive Bayes:
P(spam|email) ∝ P(spam) × ∏ P(word|spam)
```

Statistical approaches generated three key concepts:
1. the algorithms are only as good as its data (data engineering)
2. success metrics became pivotal to performance (benchmarking)
3. precision vs recall

### Shallow Learning
When two or less processing levels are used, shallow learning is typically at play. This was made during the time of KNN, decision trees, and linear regression. 
```
1. Manual Feature Extraction
  - SIFT (Scale-Invariant Feature Transform)
  - HOG (Histogram of Oriented Gradients)
  - Gabor filters
2. Feature Selection/Engineering
3. "Shallow" Learning Model (e.g., SVM)
4. Post-processing
```

### Deep Learning
While shallow learning used mathematical transformations to data, deep learning mimics brain neurons. Deep learning discovers useful features for itself from raw data, making itself very useful to complex and real-world datasets. 

As you feed a network more data, it gets exceptionally capable at solving complex tasks related to the data its fed, but it needs a lot of GPUs. Progression in this field required the three ML components. 

[![](https://mlsysbook.ai/book/contents/core/introduction/introduction_files/mediabag/7fe5c375ca4b6b91e493a3bcd4ce3a3debb9bfc1.svg)](https://mlsysbook.ai/book/contents/core/introduction/7fe5c375ca4b6b91e493a3bcd4ce3a3debb9bfc1.svg)
ML systems usually work in continuous cycles rather than a sequential linear progression. Continuous improvement usually is good for good models. Deterministic code-based systems cannot manage these data-dependent systems.

Large models are run on data centers on then on the cloud, but small models can be run on microcontrollers and embedded devices. 

### AI Engineering
[![](https://mlsysbook.ai/book/contents/core/introduction/images/png/book_pillars.png)](https://mlsysbook.ai/book/contents/core/introduction/images/png/book_pillars.png)
> **AI engineering** is the discipline on the systems-level integration an algorithms, data and compute to build and operate *reliable, efficient, and scalable* systems.

There are five AI Engineering disciplines:
1. **data engineering**: addresses data-related challenges by building robust data pipelines for quality, privacy and scale. 
2. **training systems**: tackles the model-related challenges by developing training systems for scale, complexity and efficiency. 
3. **deployment infrastructure**: addresses system-related challenges by building infrastructures for scale, reliability and flexibility.
4. **operations and monitoring**: addresses silent degradation patterns by creating monitoring systems that detect early issues and support safe updates.
5. **ethics and governance**: utterly useless.

